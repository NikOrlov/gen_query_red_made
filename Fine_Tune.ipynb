{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ee027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 19:39:53.438083: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 19:39:53.943583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import T5ForConditionalGeneration, AdamW\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc5b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vk = 'big_dev_docs/dev_'\n",
    "\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import evaluate\n",
    "\n",
    "vk_qrels = pd.read_csv(vk + 'qrels.tsv', names=['id', 'query_id', 'doc_id'],  sep='\\t')\n",
    "vk_docs = pd.read_csv(vk + 'docs.tsv', names=['id', 'doc_id', 'data'],  sep='\\t')\n",
    "vk_queries = pd.read_csv(vk + 'queries.tsv', names=['id', 'query_id', 'data'],  sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daeb40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_joined_file(df_docs, df_qrels, df_queries, path_processed_joined=None):\n",
    "    joined_df = df_qrels.merge(df_queries, on='query_id').merge(df_docs, on='doc_id', how='left')[['query_id', 'data_x', 'doc_id', 'data_y']]\n",
    "    joined_df.rename(columns={'data_x':'query_data', 'data_y':'doc_data'}, inplace=True)\n",
    "    if path_processed_joined:\n",
    "        joined_df.to_csv(path_processed_joined, sep='\\t', index=None, header=None)\n",
    "    return joined_df\n",
    "\n",
    "vk_joined = create_joined_file(vk_docs, vk_qrels, vk_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5473f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vk_joined = vk_joined[['query_data', 'doc_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "555d8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "INPUT_MAX_LEN = 100 # Input length\n",
    "OUT_MAX_LEN = 128 # Output Length\n",
    "TRAIN_BATCH_SIZE = 2 # Training Batch Size\n",
    "VALID_BATCH_SIZE = 2 # Validation Batch Size\n",
    "EPOCHS = 5 # Number of Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f89dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_token: </s> and id: 1\n",
      "unk_token: <unk> and id: 1\n",
      "pad_token: <pad> and id: 1\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, model_max_length= INPUT_MAX_LEN)\n",
    "print(\"eos_token: {} and id: {}\".format(tokenizer.eos_token, tokenizer.eos_token_id)) # End of token (eos_token)\n",
    "print(\"unk_token: {} and id: {}\".format(tokenizer.unk_token, tokenizer.eos_token_id)) # Unknown token (unk_token)\n",
    "print(\"pad_token: {} and id: {}\".format(tokenizer.pad_token, tokenizer.eos_token_id)) # Pad token (pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f5ec362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class T5Dataset(Dataset):\n",
    "    def __init__(self, documents, queries, transforms=None):\n",
    "        self.documents = documents\n",
    "        self.queries = queries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_max_length = INPUT_MAX_LEN\n",
    "        self.out_max_length = OUT_MAX_LEN\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        document = str(self.documents[idx])\n",
    "        query = str(self.queries[idx])\n",
    "        \n",
    "        inputs_encoding = self.tokenizer(\n",
    "            document, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.input_max_length,\n",
    "            padding='max_length', \n",
    "            truncation='only_first',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        output_encoding = self.tokenizer(\n",
    "            query, \n",
    "            None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.out_max_length,\n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'        \n",
    "        )\n",
    "        inputs_ids = inputs_encoding['input_ids'].flatten()\n",
    "        attention_mask = inputs_encoding['attention_mask'].flatten()\n",
    "        labels = output_encoding['input_ids']\n",
    "        \n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        labels = labels.flatten()\n",
    "        \n",
    "        out = {\n",
    "            \"document\": document,\n",
    "            \"query\": query,\n",
    "            \"inputs_ids\": inputs_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3039df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = vk_joined[['doc_data', 'query_data']]\n",
    "\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "test_dataset = dataset[train_size:]\n",
    "test_dataset = test_dataset.reset_index(drop=True)\n",
    "\n",
    "num_workers = 4\n",
    "\n",
    "train_dataset = T5Dataset(train_dataset.doc_data, train_dataset.query_data)\n",
    "val_dataset = T5Dataset(test_dataset.doc_data, test_dataset.query_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45e01dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['document', 'query', 'inputs_ids', 'attention_mask', 'labels'])\n",
      "tensor([[    3,     2,  2795,     2, 14142,  5345,  5345,     2,  7948,  6609,\n",
      "         12095,     3,  2533, 10458,  7948,     2, 27616,     3,  2533, 10458,\n",
      "          7948,     2, 25873,     2,     3,  6469,  9890,     2,  2044,     2,\n",
      "         10338,     2,  2795,  6588,     2,  7184,     2,  1757,     3,  2795,\n",
      "             3,  5814,  2795,  2533,     2, 31585,  2795,     2,  1757, 30610,\n",
      "          1757,     3, 25157,  5814,  2795,     2, 22420, 30610,  1757,     3,\n",
      "             2, 22123,  6725,  8452,     2,     3,  6469,  9890,     2,  2044,\n",
      "             2, 10338,     2,  2795,  6588,     2,  7184,     2,     3, 25157,\n",
      "          5814,  2795,     2, 22420, 30610,     2,     3,     2, 22123,  6725,\n",
      "          8452,     3, 21044, 12377,     2,  1757,  6588,     2,     3,     1],\n",
      "        [    3,     2, 26672,  9592, 14982,  6588,     3,  5814,  6588,     2,\n",
      "             3,  3700, 20000,  6588, 21302,     2,     3,     2,  2533,     2,\n",
      "          2795,  6588, 21044,  6469,     3, 20000, 15517,     3,  2044,     2,\n",
      "         31545,  6588, 21302,     2,  1421,  2122,  2122,     3,     2, 26672,\n",
      "          9592, 14982,  6588,     3, 17059,     3,     2,  2533,     2,  2795,\n",
      "          6588, 21044,  6469,     3,   318,     3,  8194, 10338,  5814,  9890,\n",
      "          1757, 12681, 30051, 24832,  2044,     3,  5814,  6588,     2,     3,\n",
      "          3700, 20000,  6588, 21302,     2,     3,  5814,  2044,     2, 10338,\n",
      "         12095,     2,  1757, 24832, 31483,     2,     3, 31545,  2044,  2044,\n",
      "             2,  7948,     2, 16811, 27616,     2,     3,  9890,     3,     1]]) tensor([[    3, 25157,  5814,  2795,     2, 22420, 30610,     2,     3,     2,\n",
      "         22123,  6725,     3, 21044, 12377,     2,  1757,  6588,     2,  8724,\n",
      "             3,  2533, 10458,  7948,     2, 27616,     1,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [    3,     2, 26672,  9592, 14982,  6588,     3, 17059,     3,     2,\n",
      "          2533,     2,  2795,  6588, 21044,  6469,     3,  2795,     2,     3,\n",
      "          2533,     2, 14982, 23110,     3, 12095,  6652,     3,     2, 14709,\n",
      "         28232,     2,  6725,     2,     1,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch.keys())\n",
    "    print(batch['inputs_ids'], batch['labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "085e864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, lables=None):\n",
    "        output = self.model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                           labels=labels)\n",
    "        \n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"inputs_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels= batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=0.0001)\n",
    "        \n",
    "       # result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2510f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('rouge')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels!= - 100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = ['\\n'.join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = ['\\n'.join(nltk.sent_tokenize(pred.strip())) for pred in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: v * 100 for k, v in result.items()}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result['gen_len'] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "def train(model, train_dataloader):\n",
    "#     device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#     model.to(device)\n",
    "    model.cuda()\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model, \n",
    "        args, \n",
    "        train_dataset=training_set,\n",
    "        eval_dataset=validation_set,\n",
    "        data_collator = data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d5fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
